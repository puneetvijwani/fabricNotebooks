{"cells":[{"cell_type":"code","source":["from azureml.opendatasets import NycTlcGreen\n","from datetime import datetime,timedelta\n","from dateutil import parser,relativedelta\n","import pyspark.sql.functions as f\n","\n","from pyspark.sql.functions import year, month, dayofmonth, dayofweek, hour,to_date\n","from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType\n","from delta.tables import DeltaTable\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:47.6012362Z","session_start_time":null,"execution_start_time":"2023-05-31T12:29:47.9610584Z","execution_finish_time":"2023-05-31T12:29:54.6334752Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"96443904-21cf-454d-954f-16742a4b798d"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 7, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{}},{"cell_type":"code","source":["%%sql \r\n","SET spark.sql.parquet.vorder.enabled=TRUE"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:47.7080698Z","session_start_time":null,"execution_start_time":"2023-05-31T12:29:54.9900333Z","execution_finish_time":"2023-05-31T12:29:55.8849464Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a2ddacf8-d254-460f-b292-b537e524b937"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 8, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":4,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"value","type":"string","nullable":false,"metadata":{}}]},"data":[["spark.sql.parquet.vorder.enabled","TRUE"]]},"text/plain":"<Spark SQL result set with 1 rows and 2 fields>"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false}},{"cell_type":"code","source":["end_date = parser.parse('2018-06-06')\r\n","start_date = parser.parse('2014-05-01')\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:47.805768Z","session_start_time":null,"execution_start_time":"2023-05-31T12:29:56.3125486Z","execution_finish_time":"2023-05-31T12:29:56.6452322Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ac5f374d-f174-43f9-b60c-6ba9af3f79a3"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 9, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["schema = StructType([\r\n","    StructField('vendorID', IntegerType(), True),\r\n","    StructField('lpepPickupDatetime', TimestampType(), True),\r\n","    StructField('lpepDropoffDatetime', TimestampType(), True),\r\n","    StructField('passengerCount', IntegerType(), True),\r\n","    StructField('tripDistance', DoubleType(), True),\r\n","    StructField('puLocationId', StringType(), True),\r\n","    StructField('doLocationId', StringType(), True),\r\n","    StructField('pickupLongitude', DoubleType(), True),\r\n","    StructField('pickupLatitude', DoubleType(), True),\r\n","    StructField('dropoffLongitude', DoubleType(), True),\r\n","    StructField('dropoffLatitude', DoubleType(), True),\r\n","    StructField('rateCodeID', IntegerType(), True),\r\n","    StructField('storeAndFwdFlag', StringType(), True),\r\n","    StructField('paymentType', IntegerType(), True),\r\n","    StructField('fareAmount', DoubleType(), True),\r\n","    StructField('extra', DoubleType(), True),\r\n","    StructField('mtaTax', DoubleType(), True),\r\n","    StructField('improvementSurcharge', StringType(), True),\r\n","    StructField('tipAmount', DoubleType(), True),\r\n","    StructField('tollsAmount', DoubleType(), True),\r\n","    StructField('ehailFee', DoubleType(), True),\r\n","    StructField('totalAmount', DoubleType(), True),\r\n","    StructField('tripType', DoubleType(), True)\r\n","   \r\n","])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:47.955642Z","session_start_time":null,"execution_start_time":"2023-05-31T12:29:57.0016056Z","execution_finish_time":"2023-05-31T12:29:57.3374183Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"6ef7592d-a809-48d9-99b6-7e3a72b59184"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 10, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["\r\n","while start_date < end_date:\r\n","    # Define the end date for this chunk (one month later)\r\n","    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\r\n","\r\n","    # Load the data for this chunk\r\n","    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\r\n","    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\r\n","    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\r\n","\r\n","    # Transform the DataFrame\r\n","    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime')) \\\r\n","        .withColumn('month', f.month('lpepPickupDatetime')) \\\r\n","        .withColumn('date', f.to_date('lpepPickupDatetime')) \\\r\n","        .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime')) \\\r\n","        .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime')) \\\r\n","        .withColumn('hour', f.hour('lpepPickupDatetime'))\r\n","\r\n","    # Save the transformed data as a Delta table, partitioned by month\r\n","    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"month\").saveAsTable(\"NYCGreenTaxi\", mode=\"append\")\r\n","\r\n","    # Update the start date for the next chunk (one month later)\r\n","    start_date = chunk_end_date\r\n","\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:48.0365175Z","session_start_time":null,"execution_start_time":"2023-05-31T12:29:57.7170708Z","execution_finish_time":"2023-05-31T12:38:26.4202834Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":270,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":279,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 49","submissionTime":"2023-05-31T12:38:26.034GMT","completionTime":"2023-05-31T12:38:26.055GMT","stageIds":[509,507,508],"jobGroup":"11","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":56,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":278,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 49","submissionTime":"2023-05-31T12:38:25.724GMT","completionTime":"2023-05-31T12:38:26.021GMT","stageIds":[506,505],"jobGroup":"11","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":277,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 49","submissionTime":"2023-05-31T12:38:25.541GMT","completionTime":"2023-05-31T12:38:25.633GMT","stageIds":[504],"jobGroup":"11","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":276,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:38:24.318GMT","completionTime":"2023-05-31T12:38:25.084GMT","stageIds":[502,503],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":275,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:38:24.193GMT","completionTime":"2023-05-31T12:38:24.281GMT","stageIds":[501],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":5266,"rowCount":50,"jobId":274,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 48","submissionTime":"2023-05-31T12:38:23.258GMT","completionTime":"2023-05-31T12:38:23.281GMT","stageIds":[499,500,498],"jobGroup":"11","status":"SUCCEEDED","numTasks":60,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":59,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5266,"dataRead":80329,"rowCount":147,"jobId":273,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 48","submissionTime":"2023-05-31T12:38:22.909GMT","completionTime":"2023-05-31T12:38:23.241GMT","stageIds":[496,497],"jobGroup":"11","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":9,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":80329,"dataRead":80331,"rowCount":194,"jobId":272,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 48","submissionTime":"2023-05-31T12:38:22.690GMT","completionTime":"2023-05-31T12:38:22.810GMT","stageIds":[495],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":9,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":9,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":18210886,"dataRead":41157628,"rowCount":1594566,"jobId":271,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:38:19.030GMT","completionTime":"2023-05-31T12:38:22.280GMT","stageIds":[494,493],"jobGroup":"11","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":41157628,"dataRead":0,"rowCount":797283,"jobId":270,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:38:18.352GMT","completionTime":"2023-05-31T12:38:18.988GMT","stageIds":[492],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":5264,"rowCount":50,"jobId":269,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 47","submissionTime":"2023-05-31T12:38:16.206GMT","completionTime":"2023-05-31T12:38:16.228GMT","stageIds":[491,489,490],"jobGroup":"11","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":58,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5264,"dataRead":77736,"rowCount":144,"jobId":268,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 47","submissionTime":"2023-05-31T12:38:15.919GMT","completionTime":"2023-05-31T12:38:16.192GMT","stageIds":[488,487],"jobGroup":"11","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":77736,"dataRead":76182,"rowCount":188,"jobId":267,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 47","submissionTime":"2023-05-31T12:38:15.649GMT","completionTime":"2023-05-31T12:38:15.818GMT","stageIds":[486],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":18076025,"dataRead":41255109,"rowCount":1600240,"jobId":266,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:38:11.998GMT","completionTime":"2023-05-31T12:38:15.188GMT","stageIds":[484,485],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":41255109,"dataRead":0,"rowCount":800120,"jobId":265,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:38:11.398GMT","completionTime":"2023-05-31T12:38:11.964GMT","stageIds":[483],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":5262,"rowCount":50,"jobId":264,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 46","submissionTime":"2023-05-31T12:37:56.693GMT","completionTime":"2023-05-31T12:37:56.715GMT","stageIds":[481,482,480],"jobGroup":"11","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":57,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5262,"dataRead":75891,"rowCount":142,"jobId":263,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 46","submissionTime":"2023-05-31T12:37:56.423GMT","completionTime":"2023-05-31T12:37:56.680GMT","stageIds":[478,479],"jobGroup":"11","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":75891,"dataRead":73864,"rowCount":184,"jobId":262,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 46","submissionTime":"2023-05-31T12:37:56.241GMT","completionTime":"2023-05-31T12:37:56.338GMT","stageIds":[477],"jobGroup":"11","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":18837500,"dataRead":43026961,"rowCount":1673938,"jobId":261,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:37:52.616GMT","completionTime":"2023-05-31T12:37:55.812GMT","stageIds":[476,475],"jobGroup":"11","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":43026961,"dataRead":0,"rowCount":836969,"jobId":260,"name":"saveAsTable at <unknown>:0","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-31T12:37:52.071GMT","completionTime":"2023-05-31T12:37:52.582GMT","stageIds":[474],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"f2f07978-5554-4197-a1db-1f552d013b9c"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 11, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Info] read from /tmp/tmpluaskkqx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=5/part-00055-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2625-2.c000.snappy.parquet\n[Info] read from /tmp/tmpluaskkqx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=6/part-00122-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2692-1.c000.snappy.parquet\n[Info] read from /tmp/tmp0ui56kpo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=6/part-00122-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2692-1.c000.snappy.parquet\n[Info] read from /tmp/tmp0ui56kpo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=7/part-00194-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2764-1.c000.snappy.parquet\n[Info] read from /tmp/tmp4rlfevhr/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=7/part-00194-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2764-1.c000.snappy.parquet\n[Info] read from /tmp/tmp4rlfevhr/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=8/part-00066-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2636-1.c000.snappy.parquet\n[Info] read from /tmp/tmpilkgov6w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=8/part-00066-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2636-1.c000.snappy.parquet\n[Info] read from /tmp/tmpilkgov6w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=9/part-00097-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2667-1.c000.snappy.parquet\n[Info] read from /tmp/tmpre4_m9tl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=10/part-00042-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2612-1.c000.snappy.parquet\n[Info] read from /tmp/tmpre4_m9tl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=9/part-00097-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2667-1.c000.snappy.parquet\n[Info] read from /tmp/tmpba19itnk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=10/part-00042-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2612-1.c000.snappy.parquet\n[Info] read from /tmp/tmpba19itnk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=11/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-1.c000.snappy.parquet\n[Info] read from /tmp/tmpdjaaxlsx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=11/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-1.c000.snappy.parquet\n[Info] read from /tmp/tmpdjaaxlsx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=12/part-00045-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2615-1.c000.snappy.parquet\n[Info] read from /tmp/tmp5oql6703/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=12/part-00045-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2615-1.c000.snappy.parquet\n[Info] read from /tmp/tmp08grnomo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=1/part-00175-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2745-1.c000.snappy.parquet\n[Info] read from /tmp/tmp08grnomo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=2/part-00007-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2577-1.c000.snappy.parquet\n[Info] read from /tmp/tmpqx9xr6er/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=2/part-00007-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2577-1.c000.snappy.parquet\n[Info] read from /tmp/tmpqx9xr6er/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=3/part-00133-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2703-1.c000.snappy.parquet\n[Info] read from /tmp/tmphn45s4rw/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=3/part-00133-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2703-1.c000.snappy.parquet\n[Info] read from /tmp/tmphn45s4rw/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=4/part-00073-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2643-1.c000.snappy.parquet\n[Info] read from /tmp/tmp_d6v85fs/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=4/part-00073-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2643-1.c000.snappy.parquet\n[Info] read from /tmp/tmp_d6v85fs/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=5/part-00177-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2747-1.c000.snappy.parquet\n[Info] read from /tmp/tmpg5izx9p0/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=5/part-00177-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2747-1.c000.snappy.parquet\n[Info] read from /tmp/tmpg5izx9p0/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=6/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-2.c000.snappy.parquet\n[Info] read from /tmp/tmp2ys44p4t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=6/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-2.c000.snappy.parquet\n[Info] read from /tmp/tmp2ys44p4t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=7/part-00147-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2717-1.c000.snappy.parquet\n[Info] read from /tmp/tmpkuktk20v/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=7/part-00147-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2717-1.c000.snappy.parquet\n[Info] read from /tmp/tmpkuktk20v/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=8/part-00074-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2644-1.c000.snappy.parquet\n[Info] read from /tmp/tmpqzt1a1ld/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=8/part-00074-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2644-1.c000.snappy.parquet\n[Info] read from /tmp/tmpqzt1a1ld/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=9/part-00092-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2662-1.c000.snappy.parquet\n[Info] read from /tmp/tmprnb5ri79/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=10/part-00105-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2675-1.c000.snappy.parquet\n[Info] read from /tmp/tmprnb5ri79/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=9/part-00092-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2662-1.c000.snappy.parquet\n[Info] read from /tmp/tmpebu1z0ff/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=10/part-00105-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2675-1.c000.snappy.parquet\n[Info] read from /tmp/tmpebu1z0ff/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=11/part-00089-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2659-1.c000.snappy.parquet\n[Info] read from /tmp/tmpmap4ua2u/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=11/part-00089-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2659-1.c000.snappy.parquet\n[Info] read from /tmp/tmpmap4ua2u/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=12/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-1.c000.snappy.parquet\n[Info] read from /tmp/tmpmvulvlgx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=12/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-1.c000.snappy.parquet\n[Info] read from /tmp/tmp6g_whe2r/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=1/part-00119-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2689-1.c000.snappy.parquet\n[Info] read from /tmp/tmp6g_whe2r/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=2/part-00060-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2630-2.c000.snappy.parquet\n[Info] read from /tmp/tmpvify3zcv/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=2/part-00060-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2630-2.c000.snappy.parquet\n[Info] read from /tmp/tmpvify3zcv/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=3/part-00196-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2766-1.c000.snappy.parquet\n[Info] read from /tmp/tmpirtfm23w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=3/part-00196-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2766-1.c000.snappy.parquet\n[Info] read from /tmp/tmpirtfm23w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=4/part-00121-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2691-1.c000.snappy.parquet\n[Info] read from /tmp/tmpiprx402n/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=4/part-00121-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2691-1.c000.snappy.parquet\n[Info] read from /tmp/tmpiprx402n/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=5/part-00044-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2614-1.c000.snappy.parquet\n[Info] read from /tmp/tmprefktjhx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=5/part-00044-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2614-1.c000.snappy.parquet\n[Info] read from /tmp/tmprefktjhx/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=6/part-00108-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2678-1.c000.snappy.parquet\n[Info] read from /tmp/tmp84zbkel6/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=6/part-00108-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2678-1.c000.snappy.parquet\n[Info] read from /tmp/tmp84zbkel6/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=7/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-2.c000.snappy.parquet\n[Info] read from /tmp/tmpnuxh2cdl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=7/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-2.c000.snappy.parquet\n[Info] read from /tmp/tmpnuxh2cdl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=8/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-2.c000.snappy.parquet\n[Info] read from /tmp/tmpo6x9zhow/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=8/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-2.c000.snappy.parquet\n[Info] read from /tmp/tmpo6x9zhow/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=9/part-00076-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2646-1.c000.snappy.parquet\n[Info] read from /tmp/tmp4n3p8r1t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=10/part-00090-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2660-1.c000.snappy.parquet\n[Info] read from /tmp/tmp4n3p8r1t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=9/part-00076-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2646-1.c000.snappy.parquet\n[Info] read from /tmp/tmprcvxt3mn/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=10/part-00090-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2660-1.c000.snappy.parquet\n[Info] read from /tmp/tmprcvxt3mn/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=11/part-00021-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2591-1.c000.snappy.parquet\n[Info] read from /tmp/tmpjqpqif2t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=11/part-00021-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2591-1.c000.snappy.parquet\n[Info] read from /tmp/tmpjqpqif2t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=12/part-00116-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2686-1.c000.snappy.parquet\n[Info] read from /tmp/tmp0_fq9ws4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=12/part-00116-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2686-1.c000.snappy.parquet\n[Info] read from /tmp/tmpd05t_y76/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=1/part-00191-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2761-1.c000.snappy.parquet\n[Info] read from /tmp/tmpd05t_y76/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=2/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-3.c000.snappy.parquet\n[Info] read from /tmp/tmp1x1uobfy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=2/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-3.c000.snappy.parquet\n[Info] read from /tmp/tmp1x1uobfy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=3/part-00010-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2580-1.c000.snappy.parquet\n[Info] read from /tmp/tmpp_chevmk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=3/part-00010-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2580-1.c000.snappy.parquet\n[Info] read from /tmp/tmpp_chevmk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=4/part-00129-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2699-1.c000.snappy.parquet\n[Info] read from /tmp/tmppvix_cpt/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=4/part-00129-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2699-1.c000.snappy.parquet\n[Info] read from /tmp/tmppvix_cpt/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=5/part-00175-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2745-2.c000.snappy.parquet\n[Info] read from /tmp/tmph7kjtqrk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=5/part-00175-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2745-2.c000.snappy.parquet\n[Info] read from /tmp/tmph7kjtqrk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=6/part-00187-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2757-1.c000.snappy.parquet\n[Info] read from /tmp/tmp6mf9f454/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=6/part-00187-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2757-1.c000.snappy.parquet\n[Info] read from /tmp/tmp6mf9f454/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=7/part-00063-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2633-1.c000.snappy.parquet\n[Info] read from /tmp/tmp3hxuscnw/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=7/part-00063-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2633-1.c000.snappy.parquet\n[Info] read from /tmp/tmp3hxuscnw/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=8/part-00014-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2584-1.c000.snappy.parquet\n[Info] read from /tmp/tmph9pdhv9w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=8/part-00014-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2584-1.c000.snappy.parquet\n[Info] read from /tmp/tmph9pdhv9w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=9/part-00102-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2672-1.c000.snappy.parquet\n[Info] read from /tmp/tmp01hii_kw/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=10/part-00018-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2588-1.c000.snappy.parquet\n[Info] read from /tmp/tmp01hii_kw/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=9/part-00102-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2672-1.c000.snappy.parquet\n[Info] read from /tmp/tmp_sz2l135/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=10/part-00018-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2588-1.c000.snappy.parquet\n[Info] read from /tmp/tmp_sz2l135/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=11/part-00174-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2744-1.c000.snappy.parquet\n[Info] read from /tmp/tmpaefrdwv4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=11/part-00174-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2744-1.c000.snappy.parquet\n[Info] read from /tmp/tmpaefrdwv4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=12/part-00096-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2666-1.c000.snappy.parquet\n[Info] read from /tmp/tmpwa9hdka6/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=12/part-00096-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2666-1.c000.snappy.parquet\n[Info] read from /tmp/tmpxfe76ki7/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=1/part-00036-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2606-1.c000.snappy.parquet\n[Info] read from /tmp/tmpxfe76ki7/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=2/part-00180-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2750-1.c000.snappy.parquet\n[Info] read from /tmp/tmpoye6stje/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=2/part-00180-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2750-1.c000.snappy.parquet\n[Info] read from /tmp/tmpoye6stje/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=3/part-00039-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2609-1.c000.snappy.parquet\n[Info] read from /tmp/tmpw12h3o1n/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=3/part-00039-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2609-1.c000.snappy.parquet\n[Info] read from /tmp/tmpw12h3o1n/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=4/part-00195-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2765-1.c000.snappy.parquet\n[Info] read from /tmp/tmpn83ikvea/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=4/part-00195-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2765-1.c000.snappy.parquet\n[Info] read from /tmp/tmpn83ikvea/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=5/part-00087-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2657-1.c000.snappy.parquet\n[Info] read from /tmp/tmpl3otu2sl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=5/part-00087-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2657-1.c000.snappy.parquet\n[Info] read from /tmp/tmpl3otu2sl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=6/part-00171-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2741-1.c000.snappy.parquet\n[Info] read from /tmp/tmp0yxb6ogz/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=6/part-00171-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2741-1.c000.snappy.parquet\n"]},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["mssparkutils.fs.ls('Tables')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:48.1676938Z","session_start_time":null,"execution_start_time":"2023-05-31T12:38:26.9593422Z","execution_finish_time":"2023-05-31T12:38:27.2977026Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"bba84262-9f23-4557-bc95-e26b7b415aa4"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 12, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"[FileInfo(path=abfss://a035ebc3-8528-4e7d-b2ba-82f859ca46be@onelake.dfs.fabric.microsoft.com/4d2adeea-ee60-4833-9d0e-e72df7bec6b9/Tables/date_managed, name=date_managed, size=0),\n FileInfo(path=abfss://a035ebc3-8528-4e7d-b2ba-82f859ca46be@onelake.dfs.fabric.microsoft.com/4d2adeea-ee60-4833-9d0e-e72df7bec6b9/Tables/holidays_managed, name=holidays_managed, size=0),\n FileInfo(path=abfss://a035ebc3-8528-4e7d-b2ba-82f859ca46be@onelake.dfs.fabric.microsoft.com/4d2adeea-ee60-4833-9d0e-e72df7bec6b9/Tables/nycgreentaxi, name=nycgreentaxi, size=0)]"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%sql\r\n","DESCRIBE DETAIL nycgreentaxi"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:29:48.3399509Z","session_start_time":null,"execution_start_time":"2023-05-31T12:38:27.7932403Z","execution_finish_time":"2023-05-31T12:38:29.4090485Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"b20ed37b-d8de-4db9-95c6-aa6a824f97e1"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 13, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"format","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"description","type":"string","nullable":true,"metadata":{}},{"name":"location","type":"string","nullable":true,"metadata":{}},{"name":"createdAt","type":"timestamp","nullable":true,"metadata":{}},{"name":"lastModified","type":"timestamp","nullable":true,"metadata":{}},{"name":"partitionColumns","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"numFiles","type":"long","nullable":true,"metadata":{}},{"name":"sizeInBytes","type":"long","nullable":true,"metadata":{}},{"name":"properties","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"minReaderVersion","type":"integer","nullable":true,"metadata":{}},{"name":"minWriterVersion","type":"integer","nullable":true,"metadata":{}}]},"data":[["delta","dd6d3a95-a931-4814-a690-91d752ef4f45","NYCTaxiLakeHouse.nycgreentaxi",null,"abfss://a035ebc3-8528-4e7d-b2ba-82f859ca46be@onelake.dfs.fabric.microsoft.com/4d2adeea-ee60-4833-9d0e-e72df7bec6b9/Tables/nycgreentaxi","2023-05-31T12:30:06Z","2023-05-31T12:38:25Z",["month"],"88","1581315857",{},1,2]]},"text/plain":"<Spark SQL result set with 1 rows and 13 fields>"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"sqlViewState":{"tableOptions":{"filterByFieldKey":"4"},"chartOptions":{"chartType":"bar","aggregationType":"avg","categoryFieldKeys":["0"],"seriesFieldKeys":["1"],"isStacked":false,"binsNumber":10}}}},{"cell_type":"code","source":["# Path to the _delta_log directory\r\n","\r\n","tablebasepath=\"abfss://YoutubeDemo@onelake.dfs.fabric.microsoft.com/NYCTaxiLakeHouse.Lakehouse/Tables/nycgreentaxi\"\r\n","tablename=f'{tablebasepath}/_delta_log'\r\n","# Get a list of all JSON files in the _delta_log directory\r\n","log_files = [file.path for file in mssparkutils.fs.ls(tablename) if file.name.endswith(\".json\")]\r\n","# Check if there are any log files\r\n","if log_files:\r\n","    # Read the first log file\r\n","    data = mssparkutils.fs.head(log_files[0])\r\n","    \r\n","    # Print the contents of the file\r\n","    print(data)\r\n","else:\r\n","    print(\"No log files found.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d28bbdc7-b92a-47a4-bac3-d44af145618a","statement_id":19,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-31T12:41:08.0038796Z","session_start_time":null,"execution_start_time":"2023-05-31T12:41:08.3144028Z","execution_finish_time":"2023-05-31T12:41:09.2479524Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"cd0d41b4-d94d-4540-aa1c-9773e7735232"},"text/plain":"StatementMeta(, d28bbdc7-b92a-47a4-bac3-d44af145618a, 19, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{\"commitInfo\":{\"timestamp\":1685536214283,\"operation\":\"CREATE TABLE AS SELECT\",\"operationParameters\":{\"isManaged\":\"true\",\"description\":null,\"partitionBy\":\"[\\\"month\\\"]\",\"properties\":\"{}\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"2\",\"numOutputRows\":\"1421543\",\"numOutputBytes\":\"38331246\"},\"tags\":{\"VORDER\":\"true\"},\"engineInfo\":\"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4\",\"txnId\":\"fe79cbba-d2a5-407c-8c3e-533b162ba5f3\"}}\n{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n{\"metaData\":{\"id\":\"dd6d3a95-a931-4814-a690-91d752ef4f45\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"vendorID\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"lpepPickupDatetime\\\",\\\"type\\\":\\\"timestamp\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"lpepDropoffDatetime\\\",\\\"type\\\":\\\"timestamp\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"passengerCount\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"tripDistance\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"puLocationId\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"doLocationId\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"pickupLongitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"pickupLatitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dropoffLongitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dropoffLatitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"rateCodeID\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"storeAndFwdFlag\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"paymentType\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"fareAmount\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"extra\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"mtaTax\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"improvementSurcharge\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"tipAmount\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"tollsAmount\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"ehailFee\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"totalAmount\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"tripType\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"year\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"month\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"day_of_month\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"day_of_week\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"hour\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[\"month\"],\"configuration\":{},\"createdTime\":1685536206545}}\n{\"add\":{\"path\":\"month=6/part-00000-94faa444-50b0-4602-af6a-15399ef7e42f.c000.snappy.parquet\",\"partitionValues\":{\"month\":\"6\"},\"size\":13029,\"modificationTime\":1685536209690,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":40,\\\"minValues\\\":{\\\"vendorID\\\":1,\\\"lpepPickupDatetime\\\":\\\"2014-06-01T00:00:00.000Z\\\",\\\"lpepDropoffDatetime\\\":\\\"2014-06-01T00:01:05.000Z\\\",\\\"passengerCount\\\":1,\\\"tripDistance\\\":0.0,\\\"pickupLongitude\\\":-73.99090576171875,\\\"pickupLatitude\\\":0.0,\\\"dropoffLongitude\\\":-73.9970474243164,\\\"dropoffLatitude\\\":0.0,\\\"rateCodeID\\\":1,\\\"storeAndFwdFlag\\\":\\\"N\\\",\\\"paymentType\\\":1,\\\"fareAmount\\\":2.5,\\\"extra\\\":0.0,\\\"mtaTax\\\":0.0,\\\"tipAmount\\\":0.0,\\\"tollsAmount\\\":0.0,\\\"totalAmount\\\":3.5,\\\"tripType\\\":1.0,\\\"year\\\":2014,\\\"date\\\":\\\"2014-06-01\\\",\\\"day_of_month\\\":1,\\\"day_of_week\\\":1,\\\"hour\\\":0},\\\"maxValues\\\":{\\\"vendorID\\\":2,\\\"lpepPickupDatetime\\\":\\\"2014-06-01T00:00:00.000Z\\\",\\\"lpepDropoffDatetime\\\":\\\"2014-06-01T23:42:23.000Z\\\",\\\"passengerCount\\\":5,\\\"tripDistance\\\":18.16,\\\"pickupLongitude\\\":0.0,\\\"pickupLatitude\\\":40.687747955322266,\\\"dropoffLongitude\\\":0.0,\\\"dropoffLatitude\\\":40.90546798706055,\\\"rateCodeID\\\":5,\\\"storeAndFwdFlag\\\":\\\"N\\\",\\\"paymentType\\\":2,\\\"fareAmount\\\":52.0,\\\"extra\\\":0.5,\\\"mtaTax\\\":0.5,\\\"tipAmount\\\":8.65,\\\"tollsAmount\\\":7.5,\\\"totalAmount\\\":57.83,\\\"tripType\\\":1.0,\\\"year\\\":2014,\\\"date\\\":\\\"2014-06-01\\\",\\\"day_of_month\\\":1,\\\"day_of_week\\\":1,\\\"hour\\\":0},\\\"nullCount\\\":{\\\"vendorID\\\":0,\\\"lpepPickupDatetime\\\":0,\\\"lpepDropoffDatetime\\\":0,\\\"passengerCount\\\":0,\\\"tripDistance\\\":0,\\\"puLocationId\\\":40,\\\"doLocationId\\\":40,\\\"pickupLongitude\\\":0,\\\"pickupLatitude\\\":0,\\\"dropoffLongitude\\\":0,\\\"dropoffLatitude\\\":0,\\\"rateCodeID\\\":0,\\\"storeAndFwdFlag\\\":0,\\\"paymentType\\\":0,\\\"fareAmount\\\":0,\\\"extra\\\":0,\\\"mtaTax\\\":0,\\\"improvementSurcharge\\\":40,\\\"tipAmount\\\":0,\\\"tollsAmount\\\":0,\\\"ehailFee\\\":40,\\\"totalAmount\\\":0,\\\"tripType\\\":0,\\\"year\\\":0,\\\"date\\\":0,\\\"day_of_month\\\":0,\\\"day_of_week\\\":0,\\\"hour\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"month=5/part-00001-26c79a82-1df7-48da-8251-15e9b0573b89.c000.snappy.parquet\",\"partitionValues\":{\"month\":\"5\"},\"size\":38318217,\"modificationTime\":1685536214135,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1421503,\\\"minValues\\\":{\\\"vendorID\\\":1,\\\"lpepPickupDatetime\\\":\\\"2014-05-01T00:00:00.000Z\\\",\\\"lpepDropoffDatetime\\\":\\\"2014-05-01T00:00:09.000Z\\\",\\\"passengerCount\\\":0,\\\"tripDistance\\\":0.0,\\\"pickupLongitude\\\":-81.25386810302734,\\\"pickupLatitude\\\":0.0,\\\"dropoffLongitude\\\":-81.24718475341797,\\\"dropoffLatitude\\\":0.0,\\\"rateCodeID\\\":1,\\\"storeAndFwdFlag\\\":\\\"N\\\",\\\"paymentType\\\":1,\\\"fareAmount\\\":0.0,\\\"extra\\\":0.0,\\\"mtaTax\\\":0.0,\\\"tipAmount\\\":0.0,\\\"tollsAmount\\\":0.0,\\\"totalAmount\\\":0.0,\\\"tripType\\\":1.0,\\\"year\\\":2014,\\\"date\\\":\\\"2014-05-01\\\",\\\"day_of_month\\\":1,\\\"day_of_week\\\":1,\\\"hour\\\":0},\\\"maxValues\\\":{\\\"vendorID\\\":2,\\\"lpepPickupDatetime\\\":\\\"2014-05-31T23:59:59.000Z\\\",\\\"lpepDropoffDatetime\\\":\\\"2014-06-01T23:50:59.000Z\\\",\\\"passengerCount\\\":9,\\\"tripDistance\\\":305.67,\\\"pickupLongitude\\\":0.0,\\\"pickupLatitude\\\":42.748878479003906,\\\"dropoffLongitude\\\":0.0,\\\"dropoffLatitude\\\":44.489013671875,\\\"rateCodeID\\\":99,\\\"storeAndFwdFlag\\\":\\\"Y\\\",\\\"paymentType\\\":5,\\\"fareAmount\\\":900.0,\\\"extra\\\":37.0,\\\"mtaTax\\\":0.5,\\\"tipAmount\\\":444.0,\\\"tollsAmount\\\":100.0,\\\"totalAmount\\\":8493.87,\\\"tripType\\\":2.0,\\\"year\\\":2014,\\\"date\\\":\\\"2014-05-31\\\",\\\"day_of_month\\\":31,\\\"day_of_week\\\":7,\\\"hour\\\":23},\\\"nullCount\\\":{\\\"vendorID\\\":0,\\\"lpepPickupDatetime\\\":0,\\\"lpepDropoffDatetime\\\":0,\\\"passengerCount\\\":0,\\\"tripDistance\\\":0,\\\"puLocationId\\\":1421503,\\\"doLocationId\\\":1421503,\\\"pickupLongitude\\\":0,\\\"pickupLatitude\\\":0,\\\"dropoffLongitude\\\":0,\\\"dropoffLatitude\\\":0,\\\"rateCodeID\\\":0,\\\"storeAndFwdFlag\\\":0,\\\"paymentType\\\":0,\\\"fareAmount\\\":0,\\\"extra\\\":0,\\\"mtaTax\\\":0,\\\"improvementSurcharge\\\":1421503,\\\"tipAmount\\\":0,\\\"tollsAmount\\\":0,\\\"ehailFee\\\":1421503,\\\"totalAmount\\\":0,\\\"tripType\\\":2,\\\"year\\\":0,\\\"date\\\":0,\\\"day_of_month\\\":0,\\\"day_of_week\\\":0,\\\"hour\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n\n"]}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"4d2adeea-ee60-4833-9d0e-e72df7bec6b9","known_lakehouses":[{"id":"4d2adeea-ee60-4833-9d0e-e72df7bec6b9"}],"default_lakehouse_name":"NYCTaxiLakeHouse","default_lakehouse_workspace_id":"a035ebc3-8528-4e7d-b2ba-82f859ca46be"}}},"nbformat":4,"nbformat_minor":0}