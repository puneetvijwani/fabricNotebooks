{"nbformat":4,"nbformat_minor":0,"metadata":{"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"4d2adeea-ee60-4833-9d0e-e72df7bec6b9","known_lakehouses":[{"id":"4d2adeea-ee60-4833-9d0e-e72df7bec6b9"}],"default_lakehouse_name":"NYCTaxiLakeHouse","default_lakehouse_workspace_id":"a035ebc3-8528-4e7d-b2ba-82f859ca46be"}}},"cells":[{"cell_type":"code","source":["from azureml.opendatasets import NycTlcGreen\n","from datetime import datetime,timedelta\n","from dateutil import parser,relativedelta\n","import pyspark.sql.functions as f\n","\n","from pyspark.sql.functions import year, month, dayofmonth, dayofweek, hour,to_date\n","from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType\n","from delta.tables import DeltaTable\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5b295501-bd16-4a61-a011-aa096709c620","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-30T16:55:10.5141421Z","session_start_time":"2023-05-30T16:55:10.7385598Z","execution_start_time":"2023-05-30T16:56:27.5514606Z","execution_finish_time":"2023-05-30T16:56:33.2986689Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"179858aa-138b-49b7-944a-79cbed7f7ca4"},"text/plain":"StatementMeta(, 5b295501-bd16-4a61-a011-aa096709c620, 5, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{}},{"cell_type":"code","source":["%%sql \r\n","SET spark.sql.parquet.vorder.enabled=TRUE"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5b295501-bd16-4a61-a011-aa096709c620","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-30T16:56:41.198174Z","session_start_time":null,"execution_start_time":"2023-05-30T16:56:41.5394809Z","execution_finish_time":"2023-05-30T16:56:42.4228401Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"937c60a3-2f38-46d5-8ddc-8e6fe3b010c0"},"text/plain":"StatementMeta(, 5b295501-bd16-4a61-a011-aa096709c620, 6, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"key","type":"string","nullable":false,"metadata":{}},{"name":"value","type":"string","nullable":false,"metadata":{}}]},"data":[["spark.sql.parquet.vorder.enabled","TRUE"]]},"text/plain":"<Spark SQL result set with 1 rows and 2 fields>"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false}},{"cell_type":"code","source":["end_date = parser.parse('2018-06-06')\r\n","start_date = parser.parse('2014-05-01')\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5b295501-bd16-4a61-a011-aa096709c620","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-30T16:56:45.7165822Z","session_start_time":null,"execution_start_time":"2023-05-30T16:56:46.0230807Z","execution_finish_time":"2023-05-30T16:56:46.3780113Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"50882128-f6e9-483d-9973-7a63db4f9a7d"},"text/plain":"StatementMeta(, 5b295501-bd16-4a61-a011-aa096709c620, 7, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["schema = StructType([\r\n","    StructField('vendorID', IntegerType(), True),\r\n","    StructField('lpepPickupDatetime', TimestampType(), True),\r\n","    StructField('lpepDropoffDatetime', TimestampType(), True),\r\n","    StructField('passengerCount', IntegerType(), True),\r\n","    StructField('tripDistance', DoubleType(), True),\r\n","    StructField('puLocationId', StringType(), True),\r\n","    StructField('doLocationId', StringType(), True),\r\n","    StructField('pickupLongitude', DoubleType(), True),\r\n","    StructField('pickupLatitude', DoubleType(), True),\r\n","    StructField('dropoffLongitude', DoubleType(), True),\r\n","    StructField('dropoffLatitude', DoubleType(), True),\r\n","    StructField('rateCodeID', IntegerType(), True),\r\n","    StructField('storeAndFwdFlag', StringType(), True),\r\n","    StructField('paymentType', IntegerType(), True),\r\n","    StructField('fareAmount', DoubleType(), True),\r\n","    StructField('extra', DoubleType(), True),\r\n","    StructField('mtaTax', DoubleType(), True),\r\n","    StructField('improvementSurcharge', StringType(), True),\r\n","    StructField('tipAmount', DoubleType(), True),\r\n","    StructField('tollsAmount', DoubleType(), True),\r\n","    StructField('ehailFee', DoubleType(), True),\r\n","    StructField('totalAmount', DoubleType(), True),\r\n","    StructField('tripType', DoubleType(), True)\r\n","   \r\n","])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5b295501-bd16-4a61-a011-aa096709c620","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-30T16:56:47.6804558Z","session_start_time":null,"execution_start_time":"2023-05-30T16:56:47.992992Z","execution_finish_time":"2023-05-30T16:56:48.3294931Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a35a83ef-6f35-46bb-979d-81af60118425"},"text/plain":"StatementMeta(, 5b295501-bd16-4a61-a011-aa096709c620, 8, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["\r\n","while start_date < end_date:\r\n","    # Define the end date for this chunk (one month later)\r\n","    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\r\n","\r\n","    # Load the data for this chunk\r\n","    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\r\n","    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\r\n","    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\r\n","\r\n","    # Transform the DataFrame\r\n","    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime')) \\\r\n","        .withColumn('month', f.month('lpepPickupDatetime')) \\\r\n","        .withColumn('date', f.to_date('lpepPickupDatetime')) \\\r\n","        .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime')) \\\r\n","        .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime')) \\\r\n","        .withColumn('hour', f.hour('lpepPickupDatetime'))\r\n","\r\n","    # Save the transformed data as a Delta table, partitioned by month\r\n","    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"month\").saveAsTable(\"NYCGreenTaxi\", mode=\"overwrite\")\r\n","\r\n","    # Update the start date for the next chunk (one month later)\r\n","    start_date = chunk_end_date\r\n","\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5b295501-bd16-4a61-a011-aa096709c620","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-30T17:01:11.4786782Z","session_start_time":null,"execution_start_time":"2023-05-30T17:01:11.8834859Z","execution_finish_time":"2023-05-30T17:10:41.7072486Z","spark_jobs":{"numbers":{"SUCCEEDED":325,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":334,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 99","submissionTime":"2023-05-30T17:10:39.555GMT","completionTime":"2023-05-30T17:10:39.575GMT","stageIds":[619,620,618],"jobGroup":"11","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":56,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":333,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 99","submissionTime":"2023-05-30T17:10:39.294GMT","completionTime":"2023-05-30T17:10:39.542GMT","stageIds":[616,617],"jobGroup":"11","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":332,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 99","submissionTime":"2023-05-30T17:10:39.069GMT","completionTime":"2023-05-30T17:10:39.170GMT","stageIds":[615],"jobGroup":"11","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":331,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:38.613GMT","completionTime":"2023-05-30T17:10:38.698GMT","stageIds":[614,613],"jobGroup":"11","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":9,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":330,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:37.857GMT","completionTime":"2023-05-30T17:10:38.553GMT","stageIds":[611,612],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":329,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:37.736GMT","completionTime":"2023-05-30T17:10:37.822GMT","stageIds":[610],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":328,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 98","submissionTime":"2023-05-30T17:10:35.768GMT","completionTime":"2023-05-30T17:10:35.790GMT","stageIds":[607,608,609],"jobGroup":"11","status":"SUCCEEDED","numTasks":60,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":59,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":327,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 98","submissionTime":"2023-05-30T17:10:35.493GMT","completionTime":"2023-05-30T17:10:35.755GMT","stageIds":[605,606],"jobGroup":"11","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":9,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":326,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 98","submissionTime":"2023-05-30T17:10:35.271GMT","completionTime":"2023-05-30T17:10:35.399GMT","stageIds":[604],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":9,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":9,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":325,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:34.803GMT","completionTime":"2023-05-30T17:10:34.872GMT","stageIds":[602,603],"jobGroup":"11","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":324,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:31.598GMT","completionTime":"2023-05-30T17:10:34.718GMT","stageIds":[601,600],"jobGroup":"11","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":323,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:31.027GMT","completionTime":"2023-05-30T17:10:31.564GMT","stageIds":[599],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":322,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 97","submissionTime":"2023-05-30T17:10:28.133GMT","completionTime":"2023-05-30T17:10:28.158GMT","stageIds":[596,597,598],"jobGroup":"11","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":58,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":321,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 97","submissionTime":"2023-05-30T17:10:27.848GMT","completionTime":"2023-05-30T17:10:28.120GMT","stageIds":[594,595],"jobGroup":"11","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":320,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 97","submissionTime":"2023-05-30T17:10:27.637GMT","completionTime":"2023-05-30T17:10:27.765GMT","stageIds":[593],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":319,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:27.154GMT","completionTime":"2023-05-30T17:10:27.226GMT","stageIds":[592,591],"jobGroup":"11","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":318,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:24.056GMT","completionTime":"2023-05-30T17:10:27.089GMT","stageIds":[589,590],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":317,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...","submissionTime":"2023-05-30T17:10:23.603GMT","completionTime":"2023-05-30T17:10:24.020GMT","stageIds":[588],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":316,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 96","submissionTime":"2023-05-30T17:10:20.383GMT","completionTime":"2023-05-30T17:10:20.403GMT","stageIds":[586,587,585],"jobGroup":"11","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":57,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":315,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n\nwhile start_date < end_date:\n    # Define the end date for this chunk (one month later)\n    chunk_end_date = min(start_date + relativedelta.relativedelta(months=1), end_date)\n\n    # Load the data for this chunk\n    nyc_tlc = NycTlcGreen(start_date=start_date, end_date=chunk_end_date)\n    nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n    nyc_tlc_df_spark = spark.createDataFrame(nyc_tlc_df, schema)\n\n    # Transform the DataFrame\n    nyc_tlc_df_transformed = nyc_tlc_df_spark.withColumn('year', f.year('lpepPickupDatetime'))         .withColumn('month', f.month('lpepPickupDatetime'))         .withColumn('date', f.to_date('lpepPickupDatetime'))         .withColumn('day_of_month', f.dayofmonth('lpepPickupDatetime'))         .withColumn('day_of_week', f.dayofweek('lpepPickupDatetime'))         .withColumn('hour', f.hour('lpepPickupDatetime'))\n\n    # Save the transformed data as a Delta table, partitioned by month\n    nyc_tlc_df_transformed.write.format('delta').option(\"overwriteSchema\", \"true\").partitionBy(\"m...: Compute snapshot for version: 96","submissionTime":"2023-05-30T17:10:20.073GMT","completionTime":"2023-05-30T17:10:20.367GMT","stageIds":[583,584],"jobGroup":"11","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"5ec1f72a-7a5a-4171-9f18-d60d765a8cfb"},"text/plain":"StatementMeta(, 5b295501-bd16-4a61-a011-aa096709c620, 11, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Info] read from /tmp/tmphwkpvsbl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=5/part-00055-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2625-2.c000.snappy.parquet\n[Info] read from /tmp/tmphwkpvsbl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=6/part-00122-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2692-1.c000.snappy.parquet\n[Info] read from /tmp/tmpk2ztyb48/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=6/part-00122-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2692-1.c000.snappy.parquet\n[Info] read from /tmp/tmpk2ztyb48/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=7/part-00194-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2764-1.c000.snappy.parquet\n[Info] read from /tmp/tmp9bfavmj8/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=7/part-00194-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2764-1.c000.snappy.parquet\n[Info] read from /tmp/tmp9bfavmj8/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=8/part-00066-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2636-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv19kr9x3/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=8/part-00066-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2636-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv19kr9x3/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=9/part-00097-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2667-1.c000.snappy.parquet\n[Info] read from /tmp/tmpb4vtf11e/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=10/part-00042-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2612-1.c000.snappy.parquet\n[Info] read from /tmp/tmpb4vtf11e/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=9/part-00097-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2667-1.c000.snappy.parquet\n[Info] read from /tmp/tmpt06ekuq4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=10/part-00042-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2612-1.c000.snappy.parquet\n[Info] read from /tmp/tmpt06ekuq4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=11/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-1.c000.snappy.parquet\n[Info] read from /tmp/tmpi1xk6gzy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=11/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-1.c000.snappy.parquet\n[Info] read from /tmp/tmpi1xk6gzy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=12/part-00045-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2615-1.c000.snappy.parquet\n[Info] read from /tmp/tmpnanqrlqe/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2014/puMonth=12/part-00045-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2615-1.c000.snappy.parquet\n[Info] read from /tmp/tmpojb2nxgs/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=1/part-00175-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2745-1.c000.snappy.parquet\n[Info] read from /tmp/tmpojb2nxgs/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=2/part-00007-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2577-1.c000.snappy.parquet\n[Info] read from /tmp/tmpelmz2rn9/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=2/part-00007-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2577-1.c000.snappy.parquet\n[Info] read from /tmp/tmpelmz2rn9/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=3/part-00133-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2703-1.c000.snappy.parquet\n[Info] read from /tmp/tmpwq9kb5rl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=3/part-00133-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2703-1.c000.snappy.parquet\n[Info] read from /tmp/tmpwq9kb5rl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=4/part-00073-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2643-1.c000.snappy.parquet\n[Info] read from /tmp/tmpmz6bn6me/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=4/part-00073-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2643-1.c000.snappy.parquet\n[Info] read from /tmp/tmpmz6bn6me/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=5/part-00177-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2747-1.c000.snappy.parquet\n[Info] read from /tmp/tmprw3k_qtv/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=5/part-00177-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2747-1.c000.snappy.parquet\n[Info] read from /tmp/tmprw3k_qtv/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=6/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-2.c000.snappy.parquet\n[Info] read from /tmp/tmp533zemsj/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=6/part-00156-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2726-2.c000.snappy.parquet\n[Info] read from /tmp/tmp533zemsj/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=7/part-00147-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2717-1.c000.snappy.parquet\n[Info] read from /tmp/tmpj259wb21/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=7/part-00147-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2717-1.c000.snappy.parquet\n[Info] read from /tmp/tmpj259wb21/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=8/part-00074-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2644-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvv8w3fs1/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=8/part-00074-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2644-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvv8w3fs1/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=9/part-00092-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2662-1.c000.snappy.parquet\n[Info] read from /tmp/tmpm18s3jf9/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=10/part-00105-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2675-1.c000.snappy.parquet\n[Info] read from /tmp/tmpm18s3jf9/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=9/part-00092-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2662-1.c000.snappy.parquet\n[Info] read from /tmp/tmpe9j8kxk_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=10/part-00105-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2675-1.c000.snappy.parquet\n[Info] read from /tmp/tmpe9j8kxk_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=11/part-00089-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2659-1.c000.snappy.parquet\n[Info] read from /tmp/tmpm34bzus7/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=11/part-00089-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2659-1.c000.snappy.parquet\n[Info] read from /tmp/tmpm34bzus7/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=12/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-1.c000.snappy.parquet\n[Info] read from /tmp/tmpk2cjczpi/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2015/puMonth=12/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-1.c000.snappy.parquet\n[Info] read from /tmp/tmpf3dm19gu/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=1/part-00119-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2689-1.c000.snappy.parquet\n[Info] read from /tmp/tmpf3dm19gu/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=2/part-00060-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2630-2.c000.snappy.parquet\n[Info] read from /tmp/tmpy9c0nkl_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=2/part-00060-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2630-2.c000.snappy.parquet\n[Info] read from /tmp/tmpy9c0nkl_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=3/part-00196-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2766-1.c000.snappy.parquet\n[Info] read from /tmp/tmp22cvi1o4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=3/part-00196-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2766-1.c000.snappy.parquet\n[Info] read from /tmp/tmp22cvi1o4/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=4/part-00121-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2691-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv1lm7tvc/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=4/part-00121-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2691-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv1lm7tvc/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=5/part-00044-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2614-1.c000.snappy.parquet\n[Info] read from /tmp/tmppwuvzm9m/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=5/part-00044-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2614-1.c000.snappy.parquet\n[Info] read from /tmp/tmppwuvzm9m/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=6/part-00108-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2678-1.c000.snappy.parquet\n[Info] read from /tmp/tmpaxijin1e/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=6/part-00108-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2678-1.c000.snappy.parquet\n[Info] read from /tmp/tmpaxijin1e/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=7/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-2.c000.snappy.parquet\n[Info] read from /tmp/tmp23m0ii85/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=7/part-00020-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2590-2.c000.snappy.parquet\n[Info] read from /tmp/tmp23m0ii85/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=8/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-2.c000.snappy.parquet\n[Info] read from /tmp/tmpy0zm2fa_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=8/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-2.c000.snappy.parquet\n[Info] read from /tmp/tmpy0zm2fa_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=9/part-00076-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2646-1.c000.snappy.parquet\n[Info] read from /tmp/tmpqv79fz_c/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=10/part-00090-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2660-1.c000.snappy.parquet\n[Info] read from /tmp/tmpqv79fz_c/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=9/part-00076-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2646-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv3jq9on3/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=10/part-00090-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2660-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv3jq9on3/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=11/part-00021-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2591-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvm70o24m/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=11/part-00021-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2591-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvm70o24m/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=12/part-00116-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2686-1.c000.snappy.parquet\n[Info] read from /tmp/tmpgsjvkmse/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2016/puMonth=12/part-00116-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2686-1.c000.snappy.parquet\n[Info] read from /tmp/tmpq39u8p75/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=1/part-00191-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2761-1.c000.snappy.parquet\n[Info] read from /tmp/tmpq39u8p75/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=2/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-3.c000.snappy.parquet\n[Info] read from /tmp/tmp0n2uhf9w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=2/part-00172-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2742-3.c000.snappy.parquet\n[Info] read from /tmp/tmp0n2uhf9w/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=3/part-00010-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2580-1.c000.snappy.parquet\n[Info] read from /tmp/tmp5o07lgjk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=3/part-00010-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2580-1.c000.snappy.parquet\n[Info] read from /tmp/tmp5o07lgjk/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=4/part-00129-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2699-1.c000.snappy.parquet\n[Info] read from /tmp/tmpr5q69y73/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=4/part-00129-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2699-1.c000.snappy.parquet\n[Info] read from /tmp/tmpr5q69y73/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=5/part-00175-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2745-2.c000.snappy.parquet\n[Info] read from /tmp/tmpuesmnikl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=5/part-00175-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2745-2.c000.snappy.parquet\n[Info] read from /tmp/tmpuesmnikl/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=6/part-00187-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2757-1.c000.snappy.parquet\n[Info] read from /tmp/tmpjvuu7y6j/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=6/part-00187-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2757-1.c000.snappy.parquet\n[Info] read from /tmp/tmpjvuu7y6j/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=7/part-00063-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2633-1.c000.snappy.parquet\n[Info] read from /tmp/tmpjcb9u55e/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=7/part-00063-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2633-1.c000.snappy.parquet\n[Info] read from /tmp/tmpjcb9u55e/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=8/part-00014-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2584-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvdq7mrj6/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=8/part-00014-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2584-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvdq7mrj6/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=9/part-00102-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2672-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvqvyt4km/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=10/part-00018-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2588-1.c000.snappy.parquet\n[Info] read from /tmp/tmpvqvyt4km/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=9/part-00102-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2672-1.c000.snappy.parquet\n[Info] read from /tmp/tmpy9irejjy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=10/part-00018-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2588-1.c000.snappy.parquet\n[Info] read from /tmp/tmpy9irejjy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=11/part-00174-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2744-1.c000.snappy.parquet\n[Info] read from /tmp/tmpfm2qtjq9/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=11/part-00174-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2744-1.c000.snappy.parquet\n[Info] read from /tmp/tmpfm2qtjq9/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=12/part-00096-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2666-1.c000.snappy.parquet\n[Info] read from /tmp/tmphc8pb_2_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2017/puMonth=12/part-00096-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2666-1.c000.snappy.parquet\n[Info] read from /tmp/tmp_m3ch17t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=1/part-00036-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2606-1.c000.snappy.parquet\n[Info] read from /tmp/tmp_m3ch17t/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=2/part-00180-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2750-1.c000.snappy.parquet\n[Info] read from /tmp/tmp1ojke7yy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=2/part-00180-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2750-1.c000.snappy.parquet\n[Info] read from /tmp/tmp1ojke7yy/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=3/part-00039-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2609-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv_2m_d1_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=3/part-00039-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2609-1.c000.snappy.parquet\n[Info] read from /tmp/tmpv_2m_d1_/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=4/part-00195-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2765-1.c000.snappy.parquet\n[Info] read from /tmp/tmp22qt9l6o/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=4/part-00195-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2765-1.c000.snappy.parquet\n[Info] read from /tmp/tmp22qt9l6o/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=5/part-00087-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2657-1.c000.snappy.parquet\n[Info] read from /tmp/tmp9hxfvpri/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=5/part-00087-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2657-1.c000.snappy.parquet\n[Info] read from /tmp/tmp9hxfvpri/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=6/part-00171-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2741-1.c000.snappy.parquet\n[Info] read from /tmp/tmpg7gjrg9f/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/green/puYear=2018/puMonth=6/part-00171-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2741-1.c000.snappy.parquet\n"]},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["mssparkutils.fs.ls('Tables')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"67c7514c-3a2b-4c80-bc31-e56c06d7da1a","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-29T17:50:53.0622664Z","session_start_time":null,"execution_start_time":"2023-05-29T17:50:53.3771588Z","execution_finish_time":"2023-05-29T17:50:53.755618Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0678df72-6d70-433b-a904-9d6d7149bf9b"},"text/plain":"StatementMeta(, 67c7514c-3a2b-4c80-bc31-e56c06d7da1a, 10, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"[FileInfo(path=abfss://a035ebc3-8528-4e7d-b2ba-82f859ca46be@onelake.dfs.fabric.microsoft.com/4d2adeea-ee60-4833-9d0e-e72df7bec6b9/Tables/nycgreentaxi, name=nycgreentaxi, size=0)]"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%sql\r\n","DESCRIBE DETAIL nycgreentaxi"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"67c7514c-3a2b-4c80-bc31-e56c06d7da1a","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-29T17:52:48.3205534Z","session_start_time":null,"execution_start_time":"2023-05-29T17:52:48.6118037Z","execution_finish_time":"2023-05-29T17:52:49.5026174Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"0e4c5d9e-08c1-4a42-b05d-27532e41df80"},"text/plain":"StatementMeta(, 67c7514c-3a2b-4c80-bc31-e56c06d7da1a, 13, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"format","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"description","type":"string","nullable":true,"metadata":{}},{"name":"location","type":"string","nullable":true,"metadata":{}},{"name":"createdAt","type":"timestamp","nullable":true,"metadata":{}},{"name":"lastModified","type":"timestamp","nullable":true,"metadata":{}},{"name":"partitionColumns","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"numFiles","type":"long","nullable":true,"metadata":{}},{"name":"sizeInBytes","type":"long","nullable":true,"metadata":{}},{"name":"properties","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"minReaderVersion","type":"integer","nullable":true,"metadata":{}},{"name":"minWriterVersion","type":"integer","nullable":true,"metadata":{}}]},"data":[["delta","b5988a41-6b47-4e8f-97b2-89b713197a75","NYCTaxiLakeHouse.nycgreentaxi",null,"abfss://a035ebc3-8528-4e7d-b2ba-82f859ca46be@onelake.dfs.fabric.microsoft.com/4d2adeea-ee60-4833-9d0e-e72df7bec6b9/Tables/nycgreentaxi","2023-05-29T13:05:36Z","2023-05-29T13:14:20Z",["month"],"88","1557738056",{},1,2]]},"text/plain":"<Spark SQL result set with 1 rows and 13 fields>"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"sqlViewState":{"tableOptions":{"filterByFieldKey":"4"},"chartOptions":{"chartType":"bar","aggregationType":"avg","categoryFieldKeys":["0"],"seriesFieldKeys":["1"],"isStacked":false,"binsNumber":10}}}},{"cell_type":"code","source":["# Path to the _delta_log directory\r\n","\r\n","tablebasepath=\"abfss://YoutubeDemo@onelake.dfs.fabric.microsoft.com/NYCTaxiLakeHouse.Lakehouse/Tables/nycgreentaxi\"\r\n","tablename=f'{tablename}/_delta_log'\r\n","# Get a list of all JSON files in the _delta_log directory\r\n","log_files = [file.path for file in mssparkutils.fs.ls(tableName) if file.name.endswith(\".json\")]\r\n","# Check if there are any log files\r\n","if log_files:\r\n","    # Read the first log file\r\n","    data = mssparkutils.fs.head(log_files[0])\r\n","    \r\n","    # Print the contents of the file\r\n","    print(data)\r\n","else:\r\n","    print(\"No log files found.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"67c7514c-3a2b-4c80-bc31-e56c06d7da1a","statement_id":58,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-29T19:16:40.8466157Z","session_start_time":null,"execution_start_time":"2023-05-29T19:16:41.2855488Z","execution_finish_time":"2023-05-29T19:16:42.1888367Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"03ad4e07-ad0f-49ce-804f-5ff84bef43d3"},"text/plain":"StatementMeta(, 67c7514c-3a2b-4c80-bc31-e56c06d7da1a, 58, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{\"commitInfo\":{\"timestamp\":1685385093977,\"operation\":\"CREATE TABLE AS SELECT\",\"operationParameters\":{\"isManaged\":\"false\",\"description\":null,\"partitionBy\":\"[]\",\"properties\":\"{}\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"8\",\"numOutputRows\":\"2284\",\"numOutputBytes\":\"91372\"},\"tags\":{\"VORDER\":\"true\"},\"engineInfo\":\"Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4\",\"txnId\":\"2f102cf5-9331-4f90-bb93-9253c0d17aae\"}}\n{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n{\"metaData\":{\"id\":\"49b311aa-a1c6-4602-94b3-c7148934eb8f\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"countryOrRegion\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"holidayName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"normalizeHolidayName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"isPaidTimeOff\\\",\\\"type\\\":\\\"boolean\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"countryRegionCode\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"timestamp\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1685385092674}}\n{\"add\":{\"path\":\"part-00000-052db223-25d8-47de-9961-3b62ac46d86b-c000.snappy.parquet\",\"partitionValues\":{},\"size\":12156,\"modificationTime\":1685385093830,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"A Munka ünnepe\\\",\\\"normalizeHolidayName\\\":\\\"A Munka ünnepe\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2014-05-01T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"秋分の日\\\",\\\"normalizeHolidayName\\\":\\\"秋分の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2014-12-25T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":271,\\\"countryRegionCode\\\":14,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00001-43c298e2-9205-4cfd-9eb5-05d55d8821cc-c000.snappy.parquet\",\"partitionValues\":{},\"size\":10735,\"modificationTime\":1685385093869,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"1. svátek vánoční\\\",\\\"normalizeHolidayName\\\":\\\"1. svátek vánoční\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2014-12-25T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"昭和の日\\\",\\\"normalizeHolidayName\\\":\\\"昭和の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2015-05-05T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":271,\\\"countryRegionCode\\\":25,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00002-69f40cdd-61c7-46bc-8723-acea897c2d92-c000.snappy.parquet\",\"partitionValues\":{},\"size\":11814,\"modificationTime\":1685385093788,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"1. svátek vánoční\\\",\\\"normalizeHolidayName\\\":\\\"1. svátek vánoční\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2015-05-06T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"秋分の日\\\",\\\"normalizeHolidayName\\\":\\\"秋分の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2015-12-26T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":269,\\\"countryRegionCode\\\":15,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00003-3091e374-5ad1-4db0-9a05-0bc6020cbb7a-c000.snappy.parquet\",\"partitionValues\":{},\"size\":11073,\"modificationTime\":1685385093803,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"2. pääsiäispäivä\\\",\\\"normalizeHolidayName\\\":\\\"2. pääsiäispäivä\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2015-12-26T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"昭和の日\\\",\\\"normalizeHolidayName\\\":\\\"昭和の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2016-05-15T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":272,\\\"countryRegionCode\\\":25,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00004-098ebc51-b691-46d0-b686-9a0fbddddbb4-c000.snappy.parquet\",\"partitionValues\":{},\"size\":11779,\"modificationTime\":1685385093828,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"1. svátek vánoční\\\",\\\"normalizeHolidayName\\\":\\\"1. svátek vánoční\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2016-05-16T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"秋分の日\\\",\\\"normalizeHolidayName\\\":\\\"秋分の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2016-12-27T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":268,\\\"countryRegionCode\\\":19,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00005-f2ef2142-86b5-4aea-9da7-47a411e54311-c000.snappy.parquet\",\"partitionValues\":{},\"size\":10890,\"modificationTime\":1685385093802,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"2. pääsiäispäivä\\\",\\\"normalizeHolidayName\\\":\\\"2. pääsiäispäivä\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2016-12-27T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"昭和の日\\\",\\\"normalizeHolidayName\\\":\\\"昭和の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2017-06-04T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":270,\\\"countryRegionCode\\\":28,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00006-002389fb-e24d-46a0-8349-bf10bfc21f9e-c000.snappy.parquet\",\"partitionValues\":{},\"size\":11847,\"modificationTime\":1685385093788,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":286,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"1. svátek vánoční\\\",\\\"normalizeHolidayName\\\":\\\"1. svátek vánoční\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2017-06-04T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"秋分の日\\\",\\\"normalizeHolidayName\\\":\\\"秋分の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2018-01-01T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":270,\\\"countryRegionCode\\\":15,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n{\"add\":{\"path\":\"part-00007-d0bfee1b-4449-46fd-b863-2b7ddfa67e07-c000.snappy.parquet\",\"partitionValues\":{},\"size\":11078,\"modificationTime\":1685385093779,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":282,\\\"minValues\\\":{\\\"countryOrRegion\\\":\\\"Argentina\\\",\\\"holidayName\\\":\\\"2. pääsiäispäivä\\\",\\\"normalizeHolidayName\\\":\\\"2. pääsiäispäivä\\\",\\\"countryRegionCode\\\":\\\"AR\\\",\\\"date\\\":\\\"2018-01-01T00:00:00.000Z\\\"},\\\"maxValues\\\":{\\\"countryOrRegion\\\":\\\"Wales\\\",\\\"holidayName\\\":\\\"昭和の日\\\",\\\"normalizeHolidayName\\\":\\\"昭和の日\\\",\\\"countryRegionCode\\\":\\\"ZA\\\",\\\"date\\\":\\\"2018-06-06T00:00:00.000Z\\\"},\\\"nullCount\\\":{\\\"countryOrRegion\\\":0,\\\"holidayName\\\":0,\\\"normalizeHolidayName\\\":0,\\\"isPaidTimeOff\\\":268,\\\"countryRegionCode\\\":21,\\\"date\\\":0}}\",\"tags\":{\"VORDER\":\"true\"}}}\n\n"]}],"execution_count":54,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}]}